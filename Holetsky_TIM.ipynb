{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1ZxdFCJagYB",
        "outputId": "b0aa63a6-5268-4c81-f45b-d63de6da7e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the size of the matrix: 300\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "  \n",
        "def is_pos_def(x):\n",
        "    return np.all(np.linalg.eigvals(x) > 0)\n",
        "\n",
        "n = int(input(\"Enter the size of the matrix: \"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_good_enough = False\n",
        "while not is_good_enough  :\n",
        "    x = np.arange(1,n)\n",
        "    V = []\n",
        "    for i in range(n):\n",
        "        y = np.random.randn(n)*2\n",
        "        V.append(y)\n",
        "    A = np.cov(V)\n",
        "    is_good_enough = is_pos_def(A)\n",
        "\n",
        "print ( is_pos_def(A))\n",
        "print(A)\n",
        "a = A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s3EBhema8dN",
        "outputId": "780003d3-938a-48fb-8ce5-2fd25a4d1f05"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "[[ 4.21756246  0.02094521 -0.15497447 ... -0.0596621   0.30874894\n",
            "   0.23012027]\n",
            " [ 0.02094521  3.77665343  0.41220205 ... -0.24153798 -0.07624279\n",
            "  -0.10118348]\n",
            " [-0.15497447  0.41220205  4.15549789 ... -0.36302578  0.21418662\n",
            "  -0.17206707]\n",
            " ...\n",
            " [-0.0596621  -0.24153798 -0.36302578 ...  3.61618206 -0.11445162\n",
            "  -0.17924071]\n",
            " [ 0.30874894 -0.07624279  0.21418662 ... -0.11445162  3.81814295\n",
            "   0.18595878]\n",
            " [ 0.23012027 -0.10118348 -0.17206707 ... -0.17924071  0.18595878\n",
            "   4.13573027]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "import numpy as np \n",
        "def MatrixLambda (n, A,i,l): \n",
        "    for p in range(n+1):\n",
        "        A[i][p]=A[i][p]*l        \n",
        "    return A\n",
        "def MatrixTwoLines (n, A,i,j): \n",
        "    for p in range(n+1):\n",
        "        A[i][p]=A[i][p]-A[j][p]\n",
        "    return A\n",
        "def gauss(a,b,n):\n",
        "    A =np.ones((n,n+1))\n",
        "    for i in range (0,n):\n",
        "        A[i][n]=b[i]\n",
        "        for j in range(0,n):\n",
        "            A[i][j]=a[i][j]\n",
        "    for k in range (n):\n",
        "        for i in range (n):\n",
        "            if A[i][k] != 0:\n",
        "                l=A[k][k]/A[i][k]\n",
        "                MatrixLambda (n, A,i,l)\n",
        "                if i !=k:\n",
        "                    MatrixTwoLines (n, A,i,k)\n",
        "    for p in range(n):\n",
        "        MatrixLambda (n, A,p,1/A[p][p])\n",
        "    return A\n",
        "b=np.ones((n,1))\n",
        "l = np.zeros((n,n))\n",
        "l[0][0] = a[0][0]**(0.5)\n",
        "h = 0\n",
        "m = 0\n",
        "for  i in range(1,n):\n",
        "    for j in range (n):\n",
        "        if j <= i:\n",
        "            h = 0\n",
        "            s = 0\n",
        "            if i == j:\n",
        "                for m in range(j):\n",
        "                    if j == 0:\n",
        "                        h = 0\n",
        "                    else:\n",
        "                        h = h + l[i][m]**2\n",
        "                l[i][j] = (a[i][j] - h)**(0.5)\n",
        "            else:\n",
        "                for m in range(j):\n",
        "                    if j == 0:\n",
        "                        s =0\n",
        "                    else:\n",
        "                        s = s + l[i][m]*l[j][m]\n",
        "                l[i][j] = (a[i][j] - s)/l[j][j]\n",
        "lt=np.transpose(l)\n",
        "y1 = gauss(l,b,n)\n",
        "y = np.zeros((n,1))\n",
        "for k in range(n):\n",
        "    y[k][0] = y1[k][n]\n",
        "x1 = gauss(lt,y,n)\n",
        "x = np.zeros((n,1))\n",
        "for k in range(n):\n",
        "    x[k][0] = x1[k][n]\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKvO-lMKa-yz",
        "outputId": "53b3471e-d923-40ef-f509-4936b32e9601"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<timed exec>:42: RuntimeWarning: invalid value encountered in double_scalars\n",
            "<timed exec>:4: RuntimeWarning: overflow encountered in double_scalars\n",
            "<timed exec>:4: RuntimeWarning: invalid value encountered in double_scalars\n",
            "<timed exec>:8: RuntimeWarning: overflow encountered in double_scalars\n",
            "<timed exec>:19: RuntimeWarning: invalid value encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "CPU times: user 41.1 s, sys: 81.7 ms, total: 41.2 s\n",
            "Wall time: 42.4 s\n"
          ]
        }
      ]
    }
  ]
}